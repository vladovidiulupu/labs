---
title: "Matrix Algebra in Practice"
author: "Rafa"
date: "January 31, 2015"
output: pdf_document
layout: page
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

# Introduciton


```{r,echo=FALSE}
set.seed(1)
```


```{r}
library(rafalib)
mypar2()
```

Here we show in more detail how we use R to fit statistical models to data. Specifically, we demonstrate how we estimate model paramaters by minimizing the residual sum of squares. The estimates are referred to as least squares estimates (LSE). We will demonstrate with the falling object example.

## Simulate the data

Thanks to my high school physics teacher (Leonardo "Panch√≥n" Morales) I know that the equation for a falling object is 

$$d = h_0 + v_0 t -  0.5 \times 9.8 t^2$$
with $h_0$ and $v_0$ the starting height and velocity respectively. We add a bit of measurement error to simulate `n` observations for dropping  the ball $(v_0=0)$ from the tower of Pisa $(h_0=56.67)$:

```{r simulate drop data}
g <- 9.8 ## meters per second
n <- 25
tt <- seq(0,3.4,len=n) ##time in secs, t is a base function
f <- 56.67  - 0.5*g*tt^2
y <-  f + rnorm(n,sd=1)
```

Here is what the data looks like with the solid line represneting the true trajectory:

```{r}
plot(tt,y,ylab="Distance in meters",xlab="Time in seconds")
lines(tt,f,col=2)
```

But now let's pretend we are Galileo and don't know the parameters in the model. The data does suggest it is a parabola so we model it like this:

$$ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon, i=1,\dots,n $$

As stated earlier, $Y_i$ represents location, $t_i$ represents time of observation, and $\varepsilon$ accounts for measurement error. 

We will estimate the model by finding the LSE or the$\beta$ s that minimize the RSS:
$$ \sum_{i=1}^n \{  Y_i - (\beta_0 + \beta_1 t_i + \beta_2 t_i^2)\}^2 $$


# The `lm` function

In R we can fit this model by simply using the `lm` function. We will describe this function in detail later, but here is a preview

```{r}
tt2 <-tt^2
fit <- lm(y~tt+tt2)
summary(fit)
```

It gives us the LSE as well as standard errors and p-values.



Let's write a function that computes the RSS for any vector $\beta$
```{r}
rss <- function(Beta0,Beta1,Beta2){
  r <- y - (Beta0+Beta1*tt+Beta2*tt^2)
  return(sum(r^2))
}
```

So for any three dimensional vector we get an RSS. Here is a plot of the RSS as a function of $\beta_2$ when we keep the other two fixed:

```{r}
Beta2s<- seq(-10,0,len=100)
plot(Beta2s,sapply(Beta2s,rss,Beta0=55,Beta1=0),
     ylab="RSS",xlab="Beta2",type="l")
```

Let's add another curve fixing another pair:

```{r}
Beta2s<- seq(-10,0,len=100)
lines(Beta2s,sapply(Beta2s,rss,Beta0=65,Beta1=0),col=2)
     
```

How do we find the three dimensional beta that minimizes this? Let's use what we learned.

First let's define $\mathbf{X}$. We use the `cbind` function that joins vectors into matrices:

```{r}
X <- cbind(1,tt,tt^2)
```

Note that R assumes that by 1 we mean a vector of 1s automatically:

```{r}
head(X)
```

We can also create a $3\times 1$ vector of $\beta$ values, multiply by $\mathbf{X}$, compute residulas and form the RSS using matrix algebra

```{r}
Beta <- matrix(c(55,0,5),3,1)
r <- y - X%*%Beta
RSS <- t(r)%*%r ### or we can use 
RSS <- crossprod(r)
RSS
```

Note this gives the same answer as the previous approach

```{r}
rss(55,0,5)
```

We did the above simply to demonstrate matrix algebra operations in R. The actual More importantly we can easily find the LSEs through the 
$$\hat{\boldsymbol{\beta}}=(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}$$ 


```{r}
betahat <- solve(t(X)%*%X)%*%t(X)%*%y
```

Note that `t` gives us the transponse and `solve` gives us the inverse. Now, the function `crossprod` makes the computation of $\mathbf{X}^\top \mathbf{Y}$ slightly faster so we instead write


```{r}
betahat <- solve(crossprod(X)) %*% crossprod(X,y)
```

Later we will show that using solve this way can be unstable and why we actually use the QR decompostion like this:

```{r}
QR <- qr(X)
betahat <- solve(qr.R(QR) , crossprod(qr.Q(QR),y))
```

More on the QR composition later.









This expression is called the residual sums of squares or RSS. Note, here we can use calculus and find the values: take the partial derivatives and set them to 0 and solve. Linear algebra provides another way of solving this problem. We will see that soon. 


## Father son's heights

```{r}
#install.packages("UsingR")
library(UsingR)
x=father.son$fheight
y=father.son$sheight
```

Now imagine you are Francis Galton in the 19th century and you collect paired height data from father and sons. You suspect that height is inherited. Your data looks like this

```{r}
plot(x,y,xlab="Father's height",ylab="Son's height")
```

The son's height does seem to increase linearly with father's height. In this case a model that describes the data is as follows:

$$ Y_i = \beta_0 + \beta_1 x_i + \varepsilon, i=1,\dots,N $$

With $x_i$ and $Y_i$ the father and son heights respectively, for the $i$-th pair and $\varepsilon$ a term to account for the extra variability. We think of the father's height as the predictor and being fixed (not random) so we use lower case.
Note that measurement error can't explain all the variability seen in $\varepsilon$. Note that this makes sense as there are other variables not in the model, for example, mother's height and environmentalism factors.

Now, how do pick $\beta_0$ and $\beta_1$ ? As before a widely used approach is to minimize the distance:

$$ \sum_{i=1}^N \{  Y_i - (\beta_0 + \beta_1 x_i)\}^2 $$

Note that this equation is similar to the one used with a dropped object data. Next we will described how linear algebra gives us a way to find the least squares estimates generally. 

## More on Galton (advanced)
When studying this data, Galton made a fascinating discovery using exploratory analysis.

<img src="http://upload.wikimedia.org/wikipedia/commons/b/b2/Galton's_correlation_diagram_1875.jpg" width=400>

He noted if he tabulated the number of father son by pairs and followed all the x,y values for which you had, say, 3 pairs it formed an ellipses. This then led to modeling this data as correlated bivariate normal. 

$$ Pr(X<a,Y<b) = \int_{-\infty}^{a} \int_{-\infty}^{b} \frac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}
\exp{ \left\{
\frac{1}{2(1-\rho^2)}
\left[\left(\frac{x-\mu_x}{\sigma_x}\right)^2 -  
2\rho\left(\frac{x-\mu_x}{\sigma_x}\right)\left(\frac{y-\mu_y}{\sigma_y}\right)+
\left(\frac{y-\mu_y}{\sigma_y}\right)^2
\right]
\right\}
}
$$

From here we can show, with some math, that if you keep $X$ fixed (condition on $x$) the the distribution of $Y$ is normally distributed with mean:
$\sigma_y \rho \left(\frac{x-\mu_x}{\sigma_x}\right)$ and standard deviation $\mu_x + \sigma_y \sqrt{1-\rho^2}$. Note that $\rho$ is the correlation between $Y$ and $X$ and this implies that conditioned of $X$, $Y$ does in fact follow a linear model. Homework what are $\beta_0$ and $\beta_1$ in terms of $\mu_x,\mu_y,\sigma_x,\sigma_y$, and $\rho$.

It turns out that the least squares estimate of $\beta_1$ can be written in terms of the sample correlation and standard deviations.

## Random samples from multiple populations

Here we read-in mouse body weight data from mice that were fed two different diets, high fat and control (chow). We have a random sample of 12 mice for each. We are interested in determining if the diet has an effect on weight. Here is the data


```{r}
dir <- system.file(package="dagdata")
filename <- file.path(dir,"extdata/femaleMiceWeights.csv")
dat <- read.csv(filename)
mypar2(1,1)
stripchart(Bodyweight~Diet,data=dat,vertical=TRUE,method="jitter",pch=1,main="Mice weights")
```

We want to estimate the difference in average weight between populations. We showed how we can use t-tests and condifence intervals based on the difference in sample averages to do this. Although linear algebrea does not actually simply the calculations here, it is worth noting that we can in fact also accomdate this data with a linear model:

$$ Y_i = \beta_0 + \beta_1 x_{i} + \varepsilon_i$$

with $\beta_0$ the chow diet average weight,
$\beta_1$ the difference between averages,
$x_i = 1$ the high fat (hf) diet,
 $x_i = 0$ the chow diet, and 
 $\varepsilon_i$ explains the differences between mice of same population.

## General linear model

Model with $p$ predictors:

$$ Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \dots +  \beta_2 x_{i,p} \varepsilon_i, i=1,\dots,n $$

 
$$ Y_i = \beta_0 + \sum_{j=1}^p \beta_j x_{i,j} + \varepsilon_i, i=1,\dots,n $$


Matrix algebra provides a compact language and mathematical framework to compute and make derivations with linear models


# Matrix Algebra Notation: the language of linear models

Linear algebra notation actually  simplifies the mathematical descriptions and manipulations of linear models as well are coding in R. We will show you the basics of this notation and then show some example in R.

The main point of this entire exercise is to show how we can write the models above using matrix notation and then explain how this is useful for solving the least squares equation. We start by simply defining notation and matrix multiplication, but bare with us, we eventually get back to the practical application.

## Motivation

Linear algebra was created by mathematicians to solve systems of linear equations such as this:

$$
\begin{aligned}
a + b + c &= 6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{aligned}
$$

It provides very useful machinery to solve these problems generally. We will learn how we can write this system using matrix algebra notation:


$$
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c\\
\end{pmatrix}
=
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
\implies
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}
=
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}^{-1}
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
$$

This section explains the notation used above. It turns that we can borrow this notation to linear models in statistics as well.




## Vectors, Matrices and Scalars

In the examples above the random variables associated with the data were represented by $Y_1,\dots,Y_n$. We can think of this as a vector. In fact, in R we are already doing this:

```{r}
library(UsingR)
y=father.son$fheight
head(y)
```
In math we can also use just one symbol and we usually use bold to distinguish it from the individual entries:

$$ \mathbf{Y} = \begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}
$$

For reasons that will become clear soon, we the default representation of data vectors has dimension $N\times 1$ as opposed to $1 \times N$.

Note: we don't always use bold because commonly one can tell what is a matrix from the context.

Similarly we can use math notation to represent the covariates or predictors. In the case of the two, with the second one just being the square of the first.

$$
\mathbf{X}_1=\begin{pmatrix}
x_{1,1}\\
\vdots\\
x_{N,1}\\
\end{pmatrix}\mbox{ and }
\mathbf{X}_2=\begin{pmatrix}
x_{1,2}\\
\vdots\\
x_{N,2}\\
\end{pmatrix}
$$

Note that, for this particular example $x_{1,1}= t_i$ and $x_{i,1}=t_i^2$ with $t_i$ the time of the i-th observation. Also note that vectors can be thought of as $N\times 1$ matrices 

For reasons that will become clear soon, it is convenient to representing  these in matrices:

$$
\mathbf{X} = [ \mathbf{X}_1 \mathbf{X_2} ] = \begin{pmatrix}
x_{1,1}&x_{1,2}\\
\vdots\\
x_{N,1}&x_{N,2}\\
\end{pmatrix}
$$

This matrix has dimensions $N \times 2$. We can create this matrix in R this way

```{r}
X <- cbind(x1=tt,x2=tt^2)
head(X)
dim(X)
```

Note that we can also use this notation to denote an arbitrary number of covariates with the following $N\times p$ matrix:

$$
\mathbf{X} = \begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} \\
  \end{pmatrix}
$$

Just as an example, we show you how to make one in R:

```{r}
N <- 100; p <- 5
X <- matrix(1:(N*p),N,p)
head(X)
dim(X)
```

Note that the columns are filled by column. The `byrow=TRUE` argument let's us change that:

```{r}
N <- 100; p <- 5
X <- matrix(1:(N*p),N,p,byrow=TRUE)
head(X)
```

Finally, we define a scalar. A scalar is just a number. So why a special name? We want to distinguish it from vectors and matrices. We usually use lower case and don't bold. In the next section we will understand why we make this distinction.


## The transpose, matrix multiplication, the identity matrix, and the inverse

We want to reiterate that at first this will all seem over-complicated but once we get to the examples you will start to appreciate its power.

### Multiplying by a scalar

The simplest operation in matrix algebra is multiplying by a scalar. If we multiply a scalar by a matrix we simply multiply each entry by that scalar:

$$
a \mathbf{X} = 
\begin{pmatrix}
  a x_{1,1} & \dots & a x_{1,p}\\
  & \vdots & \\
  a x_{N,1} & \dots & a  x_{N,p}
\end{pmatrix}
$$

Note that R automatically follows this rule when we multiple a number by a matrix:

```{r}
a <- 2
X <- matrix(1:12,4,3)
a*X
```

### The Transpose

The transpose is an operation that simply changes columns to rows. We use either a $T$ or $'$ to denote transpose.  Here is the technical definition. If X is as we defined it above, here is the transpose which will be $p\times N$:

$$
\mathbf{X}^\top = 
\begin{pmatrix}
  x_{1,1}&\dots & x_{p,1} \\
  x_{1,2}&\dots & x_{p,2} \\
   & \vdots & \\
  x_{1,N}&\dots & x_{p,N} \\
  \end{pmatrix}
$$

In R we simply type 
```{r}
X <- matrix(1:12,4,3)
X
t(X)
```

### Matrix multiplication

Now we can understand the equations we showed to motivated linear algebra:


$$
\begin{aligned}
a + b + c &=6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1\\
\end{aligned}
$$

can be written like this:

$$
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c\\
\end{pmatrix}
=
\begin{pmatrix}
a + b + c \\
3a - 2b + c \\
2a + b  - c 
\end{pmatrix}
$$



Here is the general definition of matrix multiplication of matrices $A$ and $X$

$$
\mathbf{AX} = \begin{pmatrix}
  a_{1,1} & a_{1,2} & \dots & a_{1,N}\\
  a_{2,1} & a_{2,2} & \dots & a_{2,N}\\
  & & \vdots & \\
  a_{M,1} & a_{M,2} & \dots & a_{M,N}\\
\end{pmatrix}
\begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} \\
  \end{pmatrix}
$$
  
$$  =
\begin{pmatrix}
  \sum_{i=1}^N a_{1,i} x_{i,1} & \dots & \sum_{i=1}^N a_{1,i} x_{i,p}\\
  & \vdots & \\
  \sum_{i=1}^N a_{M,i} x_{i,1} & \dots & \sum_{i=1}^N a_{M,i} x_{i,p}
\end{pmatrix}
$$

Note that you can only take the produce if the number of columns of the first matrix $A$ equals the number of rows of the second one $X$, and that the final matrix has the same row numbers as the first $A$ and the same column numbers as the second $X$. 
After you study the example below you may want to come back and re-read the sections above.

### The identity matrix

The identity matrix is analogous to the number 1: if you multiply the identity matrix you get the same matrix. For this top happen we need it to be like this:

$$
\mathbf{I} = \begin{pmatrix}
1&0&0&\dots&0&0\\
0&1&0&\dots&0&0\\
0&0&1&\dots&0&0\\
\vdots &\vdots & \vdots&\ddots&\vdots&\vdots\\
0&0&0&\dots&1&0\\
0&0&0&\dots&0&1\\
\end{pmatrix}
$$

Note that by this definition the identity always has to have the same number of rows as columns or what we call a square matrix.

If you follow the matrix multiplication rule above you notice this works out:

$$
\mathbf{XI} = 
\begin{pmatrix}
  a x_{1,1} & \dots & a x_{1,p}\\
  & \vdots & \\
  a x_{N,1} & \dots & a  x_{N,p}
\end{pmatrix}
\begin{pmatrix}
1&0&0&\dots&0&0\\
0&1&0&\dots&0&0\\
0&0&1&\dots&0&0\\
 & & &\vdots& &\\
0&0&0&\dots&1&0\\
0&0&0&\dots&0&1\\
\end{pmatrix}
= 
\begin{pmatrix}
   x_{1,1} & \dots &  x_{1,p}\\
  & \vdots & \\
   x_{N,1} & \dots & x_{N,p}
\end{pmatrix}
$$


<b> Optional homework</a>: work out the details.

In R you can form the identity this way:
```{r}
diag(5)
```

### The inverse

The inverse of matrix of $X$, denoted with $X^{-1}$ has the property that when multiplied give you the identity$X^{-1}X=I$. Note that not all matrices have inverses.
As we will see being able to compute the inverse of a matrix is quite useful. 

A very convenient aspect of R is that it includes a predefined function `solve` to do this. Here is how would use it to solve the linear of equations.

```{r}
X <- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)
y <- matrix(c(6,2,1),3,1)
solve(X)%*%y ##equivalent to solve(X,y)
```


Note: `solve` is a function that should be used with caution. One reason is that if fed a very large matrix it can take  a long time. Another certain properties of the matrix, such as using very different scales for the different columns, can make the algorithm fail, but not necessarily throw an error. To learn a more stable technique learn about the QR decomposition: `?qr

### Examples

To compute the sample average and variance of our data we use these formulas $\bar{Y}=\frac{1}{N} Y_i$ and $\mbox{var}(Y)=\frac{1}{N} \sum_{i=1}^N (Y_i - \bar{Y})^2$ with $\bar{Y}$ the average. We can represent these with matrix multiplication. First define this $N \times 1$ matrix made just of 1s

$$
A=\begin{pmatrix}
1\\
1\\
\vdots\\
1
\end{pmatrix}
$$

This implies that (note that we are multiplying by the scalar $1/N$)

$$
\frac{1}{N}
\mathbf{A}^\top Y = \frac{1}{N}
\begin{pmatrix}1&1&,\dots&1\end{pmatrix}
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}
=
\frac{1}{N} \sum_{i=1}^N Y_i
= \bar{Y}
$$

In R we multiply matrix using `%*%`

```{r}
y <- father.son$sheight
print(mean(y))

N <- length(y)
Y<- matrix(y,N,1)
A <- matrix(1,N,1)
barY=t(A)%*%Y / N

print(barY)
```

As we will see later, multiplying the transpose of a matrix with another is very common in statistics. So common there is a function in R

```{r}
bary=crossprod(A,Y) / N
print(barY)
```

For the variance we note that if

$$
\mathbf{r}=\begin{pmatrix}
Y_1 - \bar{Y}\\
\vdots\\ 
Y_N - \bar{Y}
\end{pmatrix}, 
\frac{1}{N} \mathbf{r}^\top\mathbf{r} = 
\frac{1}{N}\sum_{i=1}^N (Y_i - \bar{Y})^2
$$
And in R (note:if you only send one matrix into `crossprod` it computes: $r^\top r$)

```{r}
r <- y - barY
crossprod(r)/N
```

Which is equivalent to 
```{r}
var(y) 
```
except for the fact that the R function `var` divides by $N-1$. 

```{r}
var(y) * (N-1) / N
```

## Linear models

Now we are ready to put all this to use. Let's start with Galton's example. If we define these matrix 

$$
\mathbf{Y} = \begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix},
\mathbf{X} = \begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix},
\mathbf{\beta} = \begin{pmatrix}
\beta_0\\
\beta_1\\
\end{pmatrix} \mbox{ and }
\mathbf{\varepsilon} = \begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$

Then we can write the model 

$$ 
Y_i = \beta_0 + \beta_1 x_i + \varepsilon, i=1,\dots,N 
$$

as 

$$
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}
= 
\begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}
\begin{pmatrix}
\beta_0\\
\beta_1\\
\end{pmatrix} +
\begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$

or simply: 
$$
\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{\varepsilon}
$$

which is a much simpler way to write it. 

<b>Optional homework</b>: write out the matrices multiplication convince yourself that this this is the case.

The RSS equation becomes simpler as well as it is the following cross-product:

$$
\mathbf{(Y-X\beta)^\top(Y-X\beta)}
$$

So now we are ready to determine which values of $\beta$ minimize the above. There are a series of rules that permit us to compute partial derivatives equations in matrix notation. The only one we need here tells us that the derivative of the above equation is:

$$
\mathbf{2 X^\top (Y - X \beta)}
$$

Note: that the RSS is like a square (multiply something by itself) and that this formula is similar to  similar to the derivative of $f(x)^2$ being $2f(x)f'(x)$. 

By equating the derivative to 0 and solving for the $\beta$ we will have our solution:

$$
\mathbf{2 X^\top (Y - X \beta)}=0
$$
$$
\mathbf{X^\top X \beta = X^\top Y   }
$$
$$
\mathbf{\beta = (X^\top X)^{-1} X^\top Y   }
$$
and we have our solution. 
We usually put a hat on the $\beta$ that solves this, $\hat{\beta}$ as it is an estimate of the "real" $\beta$ that generated the data.

Let's see how it works in R

```{r}
library(UsingR)
x=father.son$fheight
y=father.son$sheight
X <- cbind(1,x)
betahat <- solve(t(X)%*%X)%*%t(X)%*%y
###or
betahat <- solve(crossprod(X))%*%crossprod(X,y)
```


Now we can see the results of this by computing the estimated $\hat{\beta}_0+\hat{\beta}_1 x$ for any value of $x$

```{r}
newx <- seq(min(x),max(x),len=100)
X <- cbind(1,newx)
fitted <- X%*%betahat
plot(x,y,xlab="Father's height",ylab="Son's height")
lines(newx,fitted,col=2)
```

This $\hat{\beta}=(X'X)^\top X^\top Y$ is one of the most widely used results in data analysis. One of the beauties of this approach is that we can use the same approach for the other problem. Note we are using almost the same exact code:


```{r}
X <- cbind(1,tt,tt^2)
y <- d
betahat <- solve(crossprod(X))%*%crossprod(X,y)
newtt <- seq(min(tt),max(tt),len=100)
X <- cbind(1,newtt,newtt^2)
fitted <- X%*%betahat
plot(tt,y,xlab="Father's height",ylab="Son's height")
lines(newtt,fitted,col=2)
```

Note the resulting estimates are what we expect:

```{r}
betahat
```

The Tower of Pisa is about 56 meters high, there is no initial velocity and half the constant of gravity is 9.8/2=4.9.

### The lm function
R has a very convenient function that fits these models. We will learn more about this function later. But here is a preview

```{r}
X <- cbind(tt,tt^2)
fit=lm(y~X)
summary(fit)
```

### The QR decomposition

If you plan on calculating $\hat{\beta}=(X'X)^\top X^\top Y$ often, and especially if you plan to work with large data sets, it serves you well to learn about the QR decomposition. You can search for the details elsewhere but here is a very useful result in R

```{r}
qr.X <- qr(X)
R <- qr.R(qr.X)
Q <- qr.Q(qr.X)
betahat <- solve(R, crossprod(Q,y) )
betahat
```
















