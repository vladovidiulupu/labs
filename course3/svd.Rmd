---
layout: page
title: Singular Value Decomposition
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

# Introduction
The main mathematical result we use to achieve dimension reduction is the singular value decomposition (SVD).
We will cover the SVD in more detail in a later section. Here we give an overview that is necessary to understand multidimensional scaling. 

# Singular Value Decomposition

The main result SVD provides is that we can write an $m \times n$ matrix $\mathbf{Y}$ as

$$\mathbf{Y = UDV^\top}$$


With:

* $\mathbf{U}$ is an $m\times n$ orthogonal matrix
* $\mathbf{V}$ is an $n\times n$ orthogonal matrix
* $\mathbf{D}$ is an $n\times n$ diagonal matrix

and with the special property that the variability (sum of squares to be precise) of the columns of $\mathbf{VD}$ and $\mathbf{UD}$ are decreasing. We will see how this  particular property turns out to be quite useful. 

If, for example, there are colinear columns the then  $\mathbf{UD}$ will include several columns with no variability. This can be seen like this
```{r}
x <- rnorm(100)
y <- rnorm(100)
z <- cbind(x,x,x,y,y)
SVD <- svd(z)
round(SVD$d,2)
```
In this case we can reconstruct `z` with just 2 columns:

```{r}
newz <- SVD$u[,1:2] %*% diag(SVD$d[1:2]) %*% t(SVD$v[,1:2])
max(abs(newz-z))
```

# How is this useful?

It is not immediately obvious how incredibly useful the SVD can be. Let's consider some examples.

First let's compute the SVD on the gene expression table we have been working with. We will take a subset so that computations are faster.
```{r}
library(tissuesGeneExpression)
data(tissuesGeneExpression)
set.seed(1)
ind <- sample(nrow(e),500)
Y <- t(apply(e[ind,],1,scale)) #standardize data for illustration
```

The `svd` command returns the three matrices (only the diagonal entries are returned for $D$)
```{r}
s <- svd(Y)
U <- s$u
V <- s$v
D <- diag(s$d) ##turn it into a matrix
```

First note that we can in fact reconstruct y

```{r}
Yhat <- U %*% D %*% t(V)
resid <- Y - Yhat
max(abs(resid))
i <- sample(ncol(Y),1)
plot(Y[,i],Yhat[,i])
abline(0,1)
boxplot(resid)
```

If we look at the sum of squares of $\mathbf{UD}$ we see that the last few are quite small. 

```{r}
plot(s$d)
```

So what happens if we remove the last column?
```{r}
k <- ncol(Y)-4
Yhat <- U[,1:k] %*% D[1:k,1:k] %*% t(V[,1:k])
resid <- Y - Yhat 
Range <- quantile(Y,c(0.01,0.99))
boxplot(resid,ylim=Range,range=0)
```

From looking at $d$, we can see that in this particular dataset we can obtain a good approximation keeping only 94 columns. The following plots are useful for seeing how much of the variability is explained by each column:

```{r}
plot(s$d^2/sum(s$d^2)*100,ylab="Percent variability explained")
```
We can also make cumulative plot

```{r}
plot(cumsum(s$d^2)/sum(s$d^2)*100,ylab="Percent variability explained",ylim=c(0,100),type="l")
```


```{r}
k <- 94
Yhat <- U[,1:k] %*% D[1:k,1:k] %*% t(V[,1:k])
resid <- Y - Yhat
boxplot(resid,ylim=Range,range=0)
```

Therefore, by using only half as many dimensions we retain most of the variability in our data:

```{r}
var(as.vector(resid))/var(as.vector(Y))
```

We say that we explain 96% of the variability.

Note that we can predict this from $D$:
```{r}
1-sum(s$d[1:k]^2)/sum(s$d^2)
```


# Highly correlated data

To help understand how the SVD works, we construct a dataset with two highly correlated columns. 

For example:

```{r}
m <- 100
n <- 2
x <- rnorm(m)
e <- rnorm(n*m,0,0.01)
Y <- cbind(x,x)+e
cor(Y)
```
In this case, the second column adds very little "information" since all the entries of `Y[,1]-Y[,2]` are close to 0. Reporting `rowMeans(Y)` is even more efficient since `Y[,1]-rowMeans(Y)` and `Y[,2]-rowMeans(Y)` are even closer to 0. `rowMeans(Y)`  turns out to be the information represented in the first column on $U$. The SVD helps us notice that we explain almost all the variability with just this first column:

```{r}
d <- svd(Y)$d
d[1]^2/sum(d^2)
```

In cases with many correlated columns we can achieve great dimension reduction:

```{r}
m <- 100
n <- 25
x <- rnorm(m)
e <- rnorm(n*m,0,0.01)
Y <- replicate(n,x)+e
d <- svd(Y)$d
d[1]^2/sum(d^2)
```





