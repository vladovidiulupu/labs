Conditional Expectations

Throughout this assessment it will be useful to remember that when our data are 0s and 1s, probabilities and expectations are the same thing. We can do the math, but here is an example in the form of R code:

```{r}
n = 1000
y = rbinom(n,1,0.25)
##proportion of ones Pr(Y)
sum(y==1)/length(y)
##expectaion of Y
mean(y)
```

 Question 2.6.1 (1 point possible)

Generate some random data to imitate heights for men (0) and women (1):

```{r}
n = 10000
set.seed(1)
men = rnorm(n,176,7) #height in centimeters
women = rnorm(n,162,7) #height in centimeters
y = c(rep(0,n),rep(1,n))
x = round(c(men,women))
##mix it up
ind = sample(seq(along=y))
y = y[ind]
x = x[ind]
```  

Treating the data generated above as the population, if we know someone is 176 cm tall, what it the probability that this person is a woman: Pr(Y=1|X=176)=E(Y|X=176)?

```{r}
mean(y[x == 176])
```

 Question 2.6.2 (1 point possible)

Now make a plot of E(Y|X=x) for x=seq(160,178) using the data generated in question 2.6.1.

Suppose for each height x you predict 1 (female) if Pr(Y|X=x)>0.5 and 0 (male) otherwise. What is the largest height for which you predict female ?

```{r}
heights <- seq(160,178)
probs <- vapply(heights, numeric(1), FUN = function(h) { mean(y[x == h]) })
plot(heights, probs, pch = 20)
abline(h = 0.5)
axis(side = 1, at = seq(160,178))
```

 Question 2.7.1 (1 point possible)

Use the data generated in question 2.6.2

```{r}
n = 10000
set.seed(1)
men = rnorm(n,176,7) #height in centimeters
women = rnorm(n,162,7) #height in centimeters
y = c(rep(0,n),rep(1,n))
x = round(c(men,women))
##mix it up
ind = sample(seq(along=y))
y = y[ind]
x = x[ind]
```

Set the seed at 5, set.seed(5) and take a random sample of 250 individuals from the population like this:

```{r}
set.seed(5)
N = 250
ind = sample(length(y),N)
Y = y[ind]
X = x[ind]
```

Use loess to estimate f(x)=E(Y|X=x) using the default parameters. What is the predicted f(168)?

```{r}
loessCurve <- loess(Y ~ X)
predict(loessCurve, newdata=data.frame(X=168))
```

Official answer:

```{r}
fit=loess(Y~X)
predict(fit,newdata=data.frame(X=168))

##Here is a plot
xs = seq(160,178)
Pr =sapply(xs,function(x0) mean(Y[X==x0]))
plot(xs,Pr)
fitted=predict(fit,newdata=data.frame(X=xs))
lines(xs,fitted)
```

 Question 2.7.2 (1 point possible)

The loess estimate above is a random variable thus we should compute its standard error. Use Monte Carlo simulation to compute the standard error of your estimate of f(168).

Set the seed to 5, set.seed(5) and perform 1000 simulation of the computations performed in question 2.7.1. Report the the SE of the loess based estimate.

```{r}
library(rafalib)
set.seed(5)

N = 250

estimates <- replicate(1000, {
  ind = sample(length(y),N)
  Y = y[ind]
  X = x[ind]
  fit <- loess(Y ~ X)
  predict(fit, newdata=data.frame(X = 168))
})
popsd(estimates)
```



Load the following dataset

```{r}
library(GSE5859Subset)
data(GSE5859Subset)
```

And define the outcome and predictors. To make the problem more difficult we will only consider autosomal genes:

```{r}
y = factor(sampleInfo$group)
X = t(geneExpression)
out = which(geneAnnotation$CHR%in%c("chrX","chrY"))
X = X[,-out]
```

Question 2.8.1 (1 point possible)

Set the seed to 1 set.seed(1) then use the createFolds function in the caret package to create 10 folds.

What is the 2nd entry in the fold 3?

```{r}
library(caret)
set.seed(1)
idx <- createFolds(y, k=10)
idx[[3]][2]
sapply(idx,function(ind) table(y[ind])) ##make sure every fold has 0s and 1s
```

 Question 2.8.2 (1 point possible)

For the following questions we are going to use kNN. We are going to consider a smaller set of predictors by filtering genes using t-tests. Specifically, we will perform a t-test and select the m genes with the smallest p-values.

Let m=8 and k=5 and train kNN by leaving out the second fold idx[[2]]

How many mistakes do we make on the test set? Remember it is indispensable that you perform the ttest on the training data.

```{r}
library(genefilter)
m <- 8
k <- 5
pvals <- rowttests(X)$p.value
topm <- order(pvals)[1:m]
Xsmall <- X[,topm]

pred <- knn(train = Xsmall[ -idx[[2]], ],
            test = Xsmall[ idx[[2]], ],
            cl = y[ -idx[[2]] ], k = k)

sum(pred != y[ idx[[2]] ])
```

 Question 2.8.3 (1 point possible)

Now run the code for question 2.8.2 for all 10 folds and keep track of the errors. What is our error rate (number of errors divided by number of predictions) ?

```{r}
m <- 8
k <- 5

mistakesCV <- sapply(1:10, function(i) {
  ind <- idx[[i]]
  Xtrain <- X[-ind,]
  pvals <- colttests(Xtrain, y[-ind])$p.value
  topm <- order(pvals)[1:m]
  Xsmall <- X[,topm]
  pred <- knn(train = Xsmall[-ind, ],
              test = Xsmall[ind, ],
              cl = y[-ind], k = k)
  mistakes <- sum(pred != y[ind])
  mistakes
})
errors <- sum(mistakesCV) / nrow(Xsmall)
```

 Question 2.8.4 (2 points possible)

Now we are going to select the best values of k and m. Use the expand grid function to try out the following values:

```{r}
ms=2^c(1:11)
ks=seq(1,9,2)
params = expand.grid(k=ks,m=ms)
```

Now use apply or a loop to obtain error rates for each of these pairs of parameters. Which pair of parameters minimizes the error rate?

```{r}
errorRates <- apply(params, 1, function(p) {
  k <- p["k"]
  m <- p["m"]
  
  mistakesCV <- sapply(1:10, function(i) {
    ind <- idx[[i]]
    Xtrain <- X[-ind,]
    pvals <- colttests(Xtrain, y[-ind])$p.value
    topm <- order(pvals)[1:m]
    Xsmall <- X[,topm]
    pred <- knn(train = Xsmall[-ind, ],
                test = Xsmall[ind, ],
                cl = y[-ind], k = k)
    mistakes <- sum(pred != y[ind])
    mistakes
  })
  
  errors <- sum(mistakesCV) / nrow(Xsmall)
  errors
})

min(errorRates)
params[which.min(errorRates),]

errors = matrix(errorRates,5,11)
library(rafalib)
mypar(1,1)
matplot(ms,t(errors),type="l",log="x")
legend("topright",as.character(ks),lty=seq_along(ks),col=seq_along(ks))
```

 Question 2.8.5 (1 point possible)

Repeat question 2.8.4 but now perform the t-test filtering before the cross validation. Note how this biases the entire result and gives us much lower estimated error rates.

What is the minimum error rate?

```{r}
errorRates <- apply(params, 1, function(p) {
  k <- p["k"]
  m <- p["m"]
  
  pvals <- colttests(X, y)$p.value
  topm <- order(pvals)[1:m]
  Xsmall <- X[,topm]
  
  mistakesCV <- sapply(1:10, function(i) {
    ind <- idx[[i]]
    
    pred <- knn(train = Xsmall[-ind, ],
                test = Xsmall[ind, ],
                cl = y[-ind], k = k)
    mistakes <- sum(pred != y[ind])
    mistakes
  })
  
  errors <- sum(mistakesCV) / nrow(Xsmall)
  errors
})

min(errorRates)
params[which.min(errorRates),]

errors = matrix(errorRates,5,11)
library(rafalib)
mypar(1,1)
matplot(ms,t(errors),type="l",log="x")
legend("topright",as.character(ks),lty=seq_along(ks),col=seq_along(ks))
```

Note how this biases the entire result and gives us much lower estimated error rates. The filtering must be applied without the test set data. 

 Question 2.8.6 (1 point possible)

Repeat the cross-validation we performed in question 2.8.4 but now instead of defining y as sampleInfo$group use:

```{r}
y = factor(as.numeric(format( sampleInfo$date, "%m")=="06"))
```

What is the minimum error rate now?

```{r}
errorRates <- apply(params, 1, function(p) {
  k <- p["k"]
  m <- p["m"]
  
  mistakesCV <- sapply(1:10, function(i) {
    ind <- idx[[i]]
    Xtrain <- X[-ind,]
    pvals <- colttests(Xtrain, y[-ind])$p.value
    topm <- order(pvals)[1:m]
    Xsmall <- X[,topm]
    pred <- knn(train = Xsmall[-ind, ],
                test = Xsmall[ind, ],
                cl = y[-ind], k = k)
    mistakes <- sum(pred != y[ind])
    mistakes
  })
  
  errors <- sum(mistakesCV) / nrow(Xsmall)
  errors
})

min(errorRates)
params[which.min(errorRates),]
```

Note that we achieve much lower error rate when predicting date than when predicting the group. Because group is confounded with date, it is very possible that these predictors have no information about group and that our lower 0.5 error rates are due to the confounding with date. We will learn more about this in the batch effect chapter.