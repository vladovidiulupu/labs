---
title: "Confidence Intervals"
layout: page
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```


```{r,results=FALSE,echo=FALSE}
set.seed(1) ##so that we get same results
```


## Confidence Intervals

We have described how to compute p-values which are ubiquitous in the life sciences. However, we do not recommend reporting p-values as the only statistical summary of your results. The reason is simple: statistical significance does not guarantee scientific significance. With large enough sample sizes, one might detect a statistically significance difference in weight of, say, 1 microgram. But is this an important finding? Would we say a diet results in higher weight if the increase is less than a fraction of a percent? The problem with reporting only p-values is that you will not provide a very important piece of information: the effect sizes.

A much more attractive alternative is to report confidence intervals. A confidence interval includes information about your estimated effect size and the uncertainty associated with this estimate. Here we use the mice data to illustrate the concept behind confidence intervals.

### Confidence Interval For Population Mean

We show how to construct a confidence interval for the population mean of control female mice. 
We start by reading in the data and selecting the appropriate rows:

```{r,echo=FALSE}
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv"
filename <- "mice_pheno.csv"
if (!file.exists(filename)) download(url,destfile=filename)
```

```{r}
dat <- read.csv("mice_pheno.csv")
chowPopulation <- dat[dat$Sex=="F" & dat$Diet=="chow",3]
```

The population average $\mu_X$ is our parameter of interest here:

```{r}
mu_chow <- mean(chowPopulation)
print(mu_chow)
```

We are interested in estimating this parameter. In practice we do not get to see the entire population so, as we did for p-values, we demonstrate how we can use samples to do this. Let's start with a sample of size 30:

```{r}
N <- 30
hf <- sample(chowPopulation,N)
```

We know this is a random variable, so the sample average will not be a perfect estimate. In fact, because in this illustrative example we know the value of the parameter, we can see that they are not exactly the same. A confidence interval is a statistical way of reporting our finding, the sample average, in a way that explicitly summarizes the variability of our random variable.

With a sample size of 30, we will use the CLT. The CLT tells us that $\bar{X}$ or `mean(hf)` follows a normal distribution with mean $\mu_X$ or `mean(chowPopulation)` and standard error approximately  $s_X/\sqrt{N}$ or:

```{r}
se <- sd(hf)/sqrt(N)
print(se)
```

<a name="interval"></a>

### Defining The Interval

A 95% confidence interval (we can use percentages other than 95%) is a random interval with a 95% probability of falling on the parameter we are estimating. To construct it we note that CLT tells us that $\sqrt{N} (\bar{X}-\mu_X)/s_X$ follows a normal distribution with mean 0 and SD 1. This implies that the probability of this event:

$$-2 \leq \sqrt{N} (\bar{X}-\mu_X)/s_X \leq 2$$  

```{r}
pnorm(2)-pnorm(-2)
```

is about 95% (to get closer use `qnorm(1-0.05/2)` instead of 2). Now do some basic algebra to clear out everything and leave $\mu_X$ alone in the middle and you get that the following event:

$$\bar{X}-2 s_X/\sqrt{N} \leq \mu_X \leq \bar{X}+2s_X/\sqrt{N}$$  

has a probability of 95%. 

Be aware that it is the edges of the interval $\bar{X} \pm 2 s_X/\sqrt{N}$, not $\mu_X$, that are random. 


What does this mean? We can construct this interval with R relatively easily:
```{r}
Q <- qnorm(1- 0.05/2)
interval <- c(mean(hf)-Q*se, mean(hf)+Q*se )
interval
```

which covers $\mu_X$ or `mean(chowPopulation)`. However, we can take another sample and we might not be as lucky. In fact, the theory tells us that we will cover $\mu_X$ 95% of the time. Because we have access to the population data, we can confirm this by taking several new samples:

```{r confidence_interval_n30,fig.cap="We show 250 random realizations of 95% confidence intervals. The color denotes if the interval fell on the parameter or not.",fig.height=8}
library(rafalib)
B <- 250
mypar()
plot(mean(chowPopulation)+c(-7,7),c(1,1),type="n",
     xlab="weight",ylab="interval",ylim=c(1,B))
abline(v=mean(chowPopulation))
for(i in 1:B){
  hf <- sample(chowPopulation,N)
  se=sd(hf)/sqrt(N)
  interval <- c(mean(hf)-Q*se, mean(hf)+Q*se )
  covered<-mean(chowPopulation)<= interval[2] & mean(chowPopulation)>=interval[1]
  color <- ifelse(covered,1,2)
  lines( interval, c(i,i),col=color)
}
```

You can run this repeatedly to see what happens. You will see that about in about 5% of the cases, we fail to cover $\mu_X$.

<a name="smallsample"></a>

### Small Sample Size And The CLT

For $N=30$ the CLT works very well. However, if $N=5$, do these confidence interval work as well? We used the CLT to create our intervals, and with $N=5$ it may not be as useful an approximation. We can confirm this with a simulation:


```{r confidence_interval_n5,fig.cap="We show 250 random realizations of 95% confidence intervals, but now for a smaller sample size. The confidence interval is based on the CLT approximation. The color denotes if the interval fell on the parameter or not.",fig.height=8}
mypar()
plot(mean(chowPopulation)+c(-7,7),c(1,1),type="n",
     xlab="weight",ylab="interval",ylim=c(1,B))
abline(v=mean(chowPopulation))
Q <- qnorm(1- 0.05/2)
N<-5
for(i in 1:B){
  hf <- sample(chowPopulation,N)
  se=sd(hf)/sqrt(N)
  interval <- c(mean(hf)-Q*se, mean(hf)+Q*se )
  covered<-mean(chowPopulation)<= interval[2] & mean(chowPopulation)>=interval[1]
  color <- ifelse(covered,1,2)
  lines( interval, c(i,i),col=color)
}
```

Despite the intervals being larger (we are dividing by $\sqrt{5}$ instead of $\sqrt{30}$), we see many more intervals not covering $\mu_X$. This is because the CLT is incorrectly telling us that the distribution of the `mean(hf)` is approximately normal when in fact it has a fatter tail. This mistake affects us in the calculation of `Q`, which assumes a normal distribution and uses `qnorm`. The t-distribution might be more appropriate. All we have to do is re-run the above, but change how we calculate `Q`: use `qt` instead of `qnorm`


```{r confidence_interval_tdist_n5,fig.cap="We show 250 random realizations of 95% confidence intervals, but now for a smaller sample size. The confidence is now based on the t-distribution approximation. The color denotes if the interval fell on the parameter or not.",fig.height=8}
mypar()
plot(mean(chowPopulation) + c(-7,7), c(1,1), type="n",
     xlab="weight", ylab="interval", ylim=c(1,B))
abline(v=mean(chowPopulation))
##Q <- qnorm(1- 0.05/2) ##no longer normal so use:
Q <- qt(1- 0.05/2, df=4)
N<-5
for(i in 1:B){
  hf <- sample(chowPopulation, N)
  se=sd(hf)/sqrt(N)
  interval <- c(mean(hf)-Q*se, mean(hf)+Q*se )
  covered<-mean(chowPopulation)<= interval[2] & mean(chowPopulation)>=interval[1]
  color <- ifelse(covered,1,2)
  lines( interval, c(i,i),col=color)
}
```

Now the intervals are made bigger. This is because the t-distribution has fatter tails and therefore:
```{r}
qt(1- 0.05/2, df=4) ##is bigger than 
qnorm(1- 0.05/2)
```

which makes the intervals larger and hence cover $\mu_X$ more frequently. In fact, about 95% of the time.


### Connection Between Confidence Intervals and p-values

We recommend that in practice confidence intervals be reported instead of p-values. If for some reason you are required to provide p-values, or required that your results are significant at the 0.05 of 0.01 levels, confidence intervals do provide this information. 

If we are talking about a t-test p-value, we are asking if the difference we observe $\bar{X} - \bar{Y}$ is actually equal to zero. So we can form a confidence interval for this difference. Instead of writing $\bar{X} - \bar{Y}$ repeatedly, let's define this difference as a new variable $\Delta \equiv \bar{X} - \bar{Y}$. The symbol $\Delta$ is often used for the difference between two variables in math and physics. 

Suppose you use CLT and report $\Delta \pm 2 s_\Delta/\sqrt{N}$ as a 95% confidence interval for the difference and this interval does not include 0. Because the interval does not include 0, this implies that either $\Delta - 2 s_\Delta/\sqrt{N}  > 0$ or $\Delta + 2 s_\Delta/\sqrt{N} < 0$. This suggests that either $\sqrt{N}\Delta/s_\Delta > 2$ or $\sqrt{N}\Delta/s_\Delta < 2$.  This then implies that the t-statistic is more extreme than 2, which in turn suggests that the p-value must be smaller than 0.05. The same calculation can be made if we use the t-distribution instead of CLT. In summary, if a 95% or 99% confidence interval does not include 0, then the p-value must be smaller than 0.05 or 0.01 respectively.

Note that the confidence interval for the difference $\Delta$ is provided by the `t.test` function:

```{r,echo=FALSE}
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv"
filename <- "femaleMiceWeights.csv"
if (!file.exists(filename)) download(url,destfile=filename)
```

```{r,echo=FALSE}
dat <- read.csv("femaleMiceWeights.csv")
controlIndex <- which(dat$Diet=="chow")
treatmentIndex <- which(dat$Diet=="hf")
control <- dat[controlIndex,2]
treatment <- dat[treatmentIndex,2]
t.test(treatment,control)
```


In this case the 95% confidence interval does include 0 and we observe that the p-value is larger than 0.05 as predicted. If we change this to a 90% confidence interval, then:

```{r}
t.test(treatment,control,conf.level=0.9)
```

0 is no longer in the confidence interval and the p-value is smaller than 0.10.

